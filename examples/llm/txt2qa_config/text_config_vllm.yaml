# TrueQuery QA Generation Configuration
# This file contains all the settings for the QA generation workflow

# Input and Output Directories
input_dir: "/Users/askliar/src/gnn/techqa/corpus"
output_dir: "/Users/askliar/src/gnn/qa_test"

# Optional: HuggingFace Dataset Download Configuration
# Uncomment to enable automatic dataset download if input_dir doesn't exist
# huggingface_repo_id: "nvidia/TechQA-RAG-Eval"
# huggingface_repo_type: "dataset"
# huggingface_files:
#   corpus: "corpus.zip"

# for modality = video, run process_keyframe_workflow to create enhanced transcription data
modality: "text"

# Backend Configuration (nim or vllm)
backend: "vllm"

# vLLM Engine Configuration
vllm_max_model_len: 32768  # Maximum context length
vllm_max_num_batched_tokens: 32768  # On CPU platforms, must be >= max_model_len

# Model Configuration
gen_model: "meta-llama/Llama-3.1-8B-Instruct"
models:
  # Choose one or more models to use for QA generation
  # mixtral: "mistralai/Mixtral-8x22B-Instruct-v0.1"
  # llama_70b: "meta-llama/Llama-3.1-70B-Instruct"
  # llama_405b: "meta-llama/Llama-3.1-405B-Instruct"
  llama_8b: "meta-llama/Llama-3.1-8B-Instruct"

model_progression:
  - "mistralai/Mixtral-8x22B-Instruct-v0.1"
  - "meta-llama/Llama-3.1-70B-Instruct"
  - "meta-llama/Llama-3.1-405B-Instruct"
  - "meta-llama/Llama-3.1-8B-Instruct"

# Embedding model for validation and deduplication
# Note: nvidia/llama-3.2-nv-embedqa-1b-v2 requires transformers>=5.0.0.dev0
embedding_model: "BAAI/bge-small-en-v1.5"

# Evaluation Configuration
evaluation:
  model: "meta-llama/Llama-3.1-8B-Instruct"
  sample_size: 15  # Number of QA pairs to evaluate if there are many
  quality_threshold: 7.0  # Minimum acceptable quality score (1-10)

use_artifact: true

# Processing Configuration
num_pairs: 7      # Number of QA pairs to generate per file
num_negatives: 2   # Number of negative answers per question (0 = none)
dedup_threshold: 0.6  # Similarity threshold for deduplication
parts: 3           # Number of parts to split transcript for diverse sampling
pairs_per_shard: 10         # Number of QA pairs each shard workflow should generate
max_parallel_shards: 6      # Maximum shard workflows to run concurrently
enable_hard_negatives: true  # Toggle hard negative mining

# Validation Settings
validation:
  min_question_length: 10
  min_answer_length: 5
  enable_span_check: true

# Question Generation Settings
self_contained_question: true  # Generate questions that don't reference the context/transcript

# Retry Configuration
retry:
  max_retries: 3
  base_delay: 1
  exponential_backoff: true

# Hard Mode Configuration (for complex questions)
hard: true
min_complexity: 4
query_type_distribution: '{"multi_hop":0.4,"structural":0.3,"contextual":0.3}'
min_hops: 2
max_hops: 4

# Optional: Distribution of reasoning types (must sum to 1.0)
# If not specified, no special handling of reasoning types
reasoning_type_distribution: '{"factual":0.15,"causal":0.15,"relational":0.15,"inferential":0.15,"temporal":0.15,"procedural":0.15,"visual":0.1}'
